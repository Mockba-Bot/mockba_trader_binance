import json
import requests
import os
import time
import sys
import re
import redis
from pydantic import BaseModel
# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from db.db_ops import  initialize_database_tables, get_bot_status
from logs.log_config import binance_trader_logger as logger
from binance.client import Client as BinanceClient
from historical_data import get_historical_data_limit_binance, get_orderbook, get_funding_rate_history, get_public_liquidations, get_funding_rate_history, get_binance_liquidation_levels

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Initialize Redis connection
redis_url = os.getenv("REDIS_URL")
if redis_url:
    try:
        redis_client = redis.from_url(redis_url)
        redis_client.ping()
        logger.info("Connected to Redis successfully")
    except redis.ConnectionError as e:
        logger.warning(f"Redis not available (optional caching disabled): {e}")
        redis_client = None
else:
    logger.info("Redis not configured (optional caching disabled)")
    redis_client = None


# Import your executor
from trading_bot.futures_executor_binance import place_futures_order, cleanup_all_orphaned_orders, recover_order_state_on_startup, start_orphan_watcher

from trading_bot.send_bot_message import send_bot_message

# Import your liquidity persistence monitor
import liquidity_persistence_monitor as lpm

RISK_PER_TRADE_PCT = float(os.getenv("RISK_PER_TRADE_PCT", "1.5"))
MAX_LEVERAGE_HIGH = int(os.getenv("MAX_LEVERAGE_HIGH", "5"))
MAX_LEVERAGE_MEDIUM = int(os.getenv("MAX_LEVERAGE_MEDIUM", "4"))
MAX_LEVERAGE_SMALL = int(os.getenv("MAX_LEVERAGE_SMALL", "3"))
MICRO_BACKTEST_MIN_EXPECTANCY = float(os.getenv("MICRO_BACKTEST_MIN_EXPECTANCY", "0.0025"))

# Request model for your signal
class TradingSignal(BaseModel):
    asset: str
    signal: str  # "LONG" or "SHORT"
    confidence: float  # 0-100%
    timeframe: str  # "4h", "1h", etc.
    current_price: float
    liquidity_score: float
    volume_1h: float
    volatility_1h: float

def get_confidence_level(confidence: float) -> str:
    """Map confidence score to human-readable level for ML signals (0-100 scale)"""
    if confidence >= 80:  # Updated for your 100% scale
        return "üöÄ VERY STRONG"
    elif confidence >= 70:
        return "üí™ STRONG"
    elif confidence >= 60:
        return "üëç MODERATE"
    else:
        return "‚ùå WEAK"

def get_leverage_by_confidence(confidence: float) -> int:
    """Get leverage based on confidence level"""
    if confidence >= 80:
        return MAX_LEVERAGE_HIGH  # High confidence = max leverage
    elif confidence >= 70:
        return MAX_LEVERAGE_MEDIUM   # Medium confidence = moderate leverage
    elif confidence >= 60:
        return MAX_LEVERAGE_SMALL   # Low confidence = low leverage
    else:
        return 1   # Very low confidence = minimal leverage

def load_prompt_template():
    """Load LLM prompt from file"""
    try:
        with open("futures_perps/trade/binance/llm_prompt_template.txt", "r") as f:
            return f.read()
    except FileNotFoundError:
        raise FileNotFoundError("llm_prompt_template.txt not found. Please create the prompt file.")

def get_current_balance():
    """Get current account balance from Binance"""
    
    client = BinanceClient(
        api_key=os.getenv("BINANCE_API_KEY"),
        api_secret=os.getenv("BINANCE_SECRET_KEY"),
        testnet=False
    )
    
    try:
        account = client.futures_account()
        for asset_info in account['assets']:
            if asset_info['asset'] == 'USDT':
                return float(asset_info['marginBalance'])
    except Exception as e:
        # Default to 20 if API call fails
        return 20.0

# Helper: Format orderbook as text (not CSV!)
def format_orderbook_as_text(ob: dict) -> str:
    lines = ["Top Bids (price, quantity):"]
    for price, qty in ob.get('bids', [])[:15]:
        lines.append(f"{price},{qty}")
    
    lines.append("\nTop Asks (price, quantity):")
    for price, qty in ob.get('asks', [])[:15]:
        lines.append(f"{price},{qty}")
    
    return "\n".join(lines)

def get_active_binance_positions_count() -> int:
    """Get count of non-zero positions from Binance Futures"""
    try:
        client = BinanceClient(
            api_key=os.getenv("BINANCE_API_KEY"),
            api_secret=os.getenv("BINANCE_SECRET_KEY"),
            testnet=False
        )
        positions = client.futures_position_information()
        active_count = sum(1 for pos in positions if float(pos['positionAmt']) != 0)
        logger.info(f"Active Binance positions: {active_count}")
        return active_count
    except Exception as e:
        logger.error(f"Failed to fetch Binance positions: {e}")
        # Fail-safe: assume 0 to avoid blocking (or return high number to block)
        return 0

def analyze_with_llm(signal_dict: dict) -> dict:
    """Send to LLM for detailed analysis using fixed prompt structure."""

    # Normalize signal direction for LLM and executor
    if signal_dict['signal'] == 1:
        signal_side_str = "BUY"
        signal_direction = "LONG"
    elif signal_dict['signal'] == -1:
        signal_side_str = "SELL"
        signal_direction = "SHORT"
    else:
        # Fallback for string inputs like "LONG"
        raw = str(signal_dict['signal']).upper()
        if 'BUY' in raw or 'LONG' in raw:
            signal_side_str = "BUY"
            signal_direction = "LONG"
        else:
            signal_side_str = "SELL"
            signal_direction = "SHORT"
    
    # ‚úÖ Get DataFrame with ALL indicators (your function handles timeframe logic)
    df = get_historical_data_limit_binance(
        pair=signal_dict['asset'],
        timeframe=signal_dict['interval'],
        limit=30
    )
    csv_content = df.to_csv(index=False)  # ‚Üê Preserves all columns automatically
    # get the latest close price from the dataframe
    latest_close_price = df['close'].iloc[-1]

    # ‚úÖ Get orderbook as TEXT (not CSV!)
    orderbook = get_orderbook(signal_dict['asset'], limit=20)
    orderbook_content = format_orderbook_as_text(orderbook)  # ‚Üê See helper below

    # Get Binance funding rate history
    funding_history = get_funding_rate_history(symbol=signal_dict['asset'], limit=50)
    
    # Calculate meaningful funding metrics from Binance data
    if funding_history and isinstance(funding_history, list):
        # Convert funding rates to float and handle Binance format
        funding_rates = []
        for item in funding_history:
            try:
                rate = float(item.get('fundingRate', 0))
                funding_rates.append(rate)
            except (ValueError, TypeError):
                funding_rates.append(0)
        
        current_funding = funding_rates[0] if funding_rates else 0
        avg_funding = sum(funding_rates) / len(funding_rates) if funding_rates else 0
        
        funding_trend = "POSITIVE" if current_funding > avg_funding else "NEGATIVE"
        funding_extreme = abs(current_funding) > 0.0005  # 0.05%
        
        # Count positive vs negative funding periods
        positive_funding = sum(1 for rate in funding_rates if rate > 0)
        negative_funding = sum(1 for rate in funding_rates if rate < 0)
        funding_bias = "ALCISTA" if positive_funding > negative_funding else "BAJISTA"
        
    else:
        current_funding = 0
        funding_trend = "UNKNOWN"
        funding_extreme = False
        funding_bias = "NEUTRO"

    # Get liquidations data (Binance format)
    liquidations = get_public_liquidations(symbol=signal_dict['asset'], lookback_hours=24)
    liquidation_levels = get_binance_liquidation_levels(symbol=signal_dict['asset'])
    
    # Analyze liquidation context
    if liquidations and isinstance(liquidations, list):
        total_liquidations = len(liquidations)
        # Binance liquidations might have different structure
        liquidation_analysis = f"Liquidaciones activas: {total_liquidations}"
    else:
        total_liquidations = 0
        liquidation_analysis = "Sin liquidaciones recientes detectadas"
    
    # Use liquidation levels data if available
    if liquidation_levels:
        open_interest = liquidation_levels.get('open_interest', 0)
        mark_price = liquidation_levels.get('mark_price', latest_close_price)
        oi_context = f"Inter√©s abierto: {open_interest:,.0f} BTC"
    else:
        oi_context = "Inter√©s abierto: No disponible"

    # Enhanced funding context with actual Binance data
    funding_context = (
        "AN√ÅLISIS DE TASA DE FINANCIAMIENTO BINANCE (datos reales):\n"
        f"‚Ä¢ Tasa actual: {current_funding:.6f} ({current_funding*10000:.2f} bps)\n"
        f"‚Ä¢ Promedio 50 periodos: {avg_funding:.6f}\n"
        f"‚Ä¢ Tendencia: {funding_trend}\n"
        f"‚Ä¢ Sesgo: {funding_bias} ({positive_funding if funding_rates else 0} pos / {negative_funding if funding_rates else 0} neg)\n"
        f"‚Ä¢ Es extremo: {'S√ç' if funding_extreme else 'NO'}\n"
        "Interpretaci√≥n Binance:\n"
        "- Funding >0: Largos pagan a cortos ‚Üí presi√≥n bajista potencial\n"
        "- Funding <0: Cortos pagan a largos ‚Üí presi√≥n alcista potencial\n"
        "- |Funding|>0.05%: Se√±al contraria fuerte\n"
        "- Tendencia persistente: Indica sentimiento direccional\n"
    )

    # Enhanced liquidation context for Binance
    liquidation_context = (
        "AN√ÅLISIS DE LIQUIDACIONES BINANCE:\n"
        f"‚Ä¢ {liquidation_analysis}\n"
        f"‚Ä¢ {oi_context}\n"
        f"‚Ä¢ Precio mark: ${mark_price if liquidation_levels else latest_close_price:.2f}\n"
        "Patrones de liquidaci√≥n Binance:\n"
        "- Alto inter√©s abierto + funding positivo = riesgo bajista\n"
        "- Alto inter√©s abierto + funding negativo = riesgo alcista\n"
        "- Liquidaciones recientes indican niveles de stop-loss activos\n"
        "- El smart money opera en contra de clusters de liquidaci√≥n\n"
    )


    # --- Rest of your prompt logic (unchanged) ---
    intro = (
        "Eres un trader discrecional de elite en futuros de cripto con m√°s de 10 a√±os de experiencia.  \n"
        "Tu trabajo es **validar o rechazar** la se√±al dada usando SOLO los datos proporcionados. .\n"
        "Analiza el CSV adjunto (80 velas) y el libro de √≥rdenes para la se√±al dada.\n\n"
        f"‚Ä¢ Asset: {signal_dict['asset']}\n"
        f"‚Ä¢ Signal: {signal_direction} ({signal_side_str})\n"
        f"‚Ä¢ Confidence: {signal_dict['confidence']}%\n"
        f"‚Ä¢ Timeframe: {signal_dict['interval']}\n"
        f"‚Ä¢ Current Price: ${latest_close_price}\n"
        f"‚Ä¢ Liquidity Score: {signal_dict['liquidity_score']}\n"
        f"‚Ä¢ Volume (1h): ${signal_dict['volume_1h']}\n"
        f"‚Ä¢ Volatility (1h): {signal_dict['volatility_1h']}%\n\n"
    )

     # Get leverage based on confidence
    leverage = get_leverage_by_confidence(signal_dict['confidence_percent'])
            

    analysis_logic = load_prompt_template()

    # Ensure LLM knows how to set price levels ‚Äî even if user prompt is vague
    fallback_price_instructions = (
        "\n\n### SI NO SE ESPECIFICA LO CONTRARIO, USA ESTAS REGLAS OBLIGATORIAS:\n"
        "- entry: usa el precio actual de mercado (√∫ltimo close del CSV)\n"
        "- stop_loss: col√≥calo M√ÅS ALL√Å del swing reciente (soporte/resistencia m√°s cercano en contra de la se√±al)\n"
        "- take_profit: aplica ratio 1:3 ‚Üí take_profit = entry ¬± 3 √ó |entry ‚àí stop_loss|\n"
        "- Aseg√∫rate de que SL y TP est√©n en el lado correcto seg√∫n la direcci√≥n (BUY/SELL)\n"
    )

    # Only add it if not already covered (optional), or just always add it for safety
    analysis_logic += fallback_price_instructions

    response_format = (
        "\nRetorna SOLAMENTE un objeto JSON v√°lido con las siguientes claves:\n"
        "- symbol: str (e.g., 'LTCUSDT')\n"
        "- side: str ('BUY' or 'SELL')\n"
        "- entry: float (use current market price as base)\n"
        "- take_profit: float\n"
        "- stop_loss: float\n"
        "- confidence: float (copy from input: " + str(signal_dict['confidence_percent']) + ")\n"
        "- leverage: int (use: " + str(leverage) + ")\n"
        "\nDo NOT include any other text, explanation, or markdown. Only pure JSON."
    )

    # Enhanced risk context with Binance-specific data
    risk_context = (
        f"\n--- PAR√ÅMETROS DE RIESGO BINANCE (OBLIGATORIOS) ---\n"
        f"‚Ä¢ Apalancamiento m√°ximo: {leverage}x\n"
        f"‚Ä¢ Riesgo/operaci√≥n: {RISK_PER_TRADE_PCT}% del balance\n"
        f"‚Ä¢ Balance disponible: ~${get_current_balance():.2f}\n"
        f"‚Ä¢ Ratio R:B obligatorio: 1:3\n"
        f"‚Ä¢ Funding actual: {current_funding:.6f} ‚Üí {'SE√ëAL CONTRARIA FUERTE' if funding_extreme else 'NEUTRO'}\n"
        f"‚Ä¢ Inter√©s abierto: {open_interest if liquidation_levels else 'N/A':,.0f}\n"
        f"‚Ä¢ Liquidaciones recientes: {total_liquidations} ‚Üí {'ALERTA VOLATILIDAD' if total_liquidations > 10 else 'NORMAL'}\n"
        f"- Si funding es extremo, considera operaci√≥n contraria\n"
        f"- Si alto inter√©s abierto, mayor potencial de movimientos\n"
        f"- Stop loss DEBE evitar niveles de liquidaci√≥n masiva\n"
    )

    additional_market_context = (
        "\n\nCONTEXTO ADICIONAL DEL MERCADO A CONSIDERAR:\n"
        "- Analiza los extremos de funding rate para oportunidades de trading contrario\n"
        "- Identifica clusters de liquidaci√≥n que pueden causar movimientos violentos\n"
        "- Combina la profundidad del orderbook con niveles de liquidaci√≥n para S/R clave\n"
        "- Usa las tendencias de funding para medir la saturaci√≥n de sentimiento del mercado\n"
    )

    prompt = intro + analysis_logic + risk_context + additional_market_context + response_format

    # Debug the prmopt
    logger.debug(f"LLM Prompt:\n{prompt[:2000]}...\n--- End of Prompt ---")

    # --- Send to DeepSeek ---
    # --- Send to DeepSeek ---
    response = requests.post(
        "https://api.deepseek.com/v1/chat/completions",
        headers={"Authorization": f"Bearer {os.getenv('DEEP_SEEK_API_KEY')}"},
        json={
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt},
                {"role": "user", "content": f"Candles (CSV format):\n{csv_content}"},
                {"role": "user", "content": f"Orderbook:\n{orderbook_content}"},
                {"role": "user", "content": funding_context},
                {"role": "user", "content": liquidation_context}
            ],
            "temperature": 0.0,
            "max_tokens": 500
        }
    )
    
    if response.status_code == 200:
        content = response.json()['choices'][0]['message']['content']
        # Check if LLM approves the trade
        approved = "DO NOT EXECUTE" not in content.upper()
        return {"analysis": content, "approved": approved}
    return {"analysis": "LLM analysis failed", "approved": False}


def process_signal():
    while True:
        """Process incoming signal from Api bot with combined CSV + orderbook file"""

        # Cleanup orphaned orders on each loop
        cleanup_all_orphaned_orders()

        # Only proceed if bot is running
        if not get_bot_status():
            logger.info("Bot is paused. Waiting to resume...")
            time.sleep(30)
            continue

        # Get signal from Mockba ML
        URL = "https://signal.globaldv.net/api/v1/signals/active?venue=CEX"
        # The API is free, get no post
        response = requests.get(URL)
        if response.status_code != 200:
            logger.error(f"Failed to fetch signals: {response.status_code}")
            time.sleep(30)
            continue

        # If the ressponse if a empty list, skip
        if response.json() == []:
            # logger.info("No active signals received.")
            time.sleep(30)
            continue
        
        signals = response.json()  # List of signal dicts
        # Emulate json
        # signals = [
        # {
        #     "asset": "LTCUSDT",
        #     "signal": 1,
        #     "confidence": 1,
        #     "confidence_percent": 100,
        #     "interval": "4h",
        #     "venue": "CEX",
        #     "score": 1,
        #     "regime": "HIGH VOLATILITY",
        #     "timestamp": "2025-11-23T22:19:30.249249+00:00",
        #     "expires_at": 1763939970.249278,
        #     "signal_id": "CEX_20251123_221930",
        #     "liquidity_tier": "Unknown",
        #     "liquidity_score": 7.28,
        #     "volume_1h": 2483583.05,
        #     "volatility_1h": 1.24,
        #     "backtest": {
        #     "trades": 131,
        #     "winrate": 0.802,
        #     "avg_ret": 0.0016,
        #     "exp": 0.0032,
        #     "max_dd": -0.0652
        #     }
        # }
        # ]

        # Compare with Redis to avoid duplicates
        if redis_client:
            current_id = signals[0].get('signal_id') if signals else None
            stored_id = redis_client.get("latest_signal_id")
            
            if stored_id and current_id == stored_id.decode('utf-8'):
                # logger.info(f"Signal {current_id} already processed. Skipping.")
                time.sleep(30)
                continue
            elif current_id:
                redis_client.setex("latest_signal_id", 3600, current_id)
        else:
            logger.warning("Redis not available, skipping deduplication")
            

        # Process the single signal (API always returns one)
        if signals:

            # ‚úÖ Enforce max 5 concurrent positions
            active_count = get_active_binance_positions_count()
            if active_count >= int(os.getenv("MAX_CONCURRENT_TRADES", "5")):
                logger.info(f"Max concurrent positions ({os.getenv('MAX_CONCURRENT_TRADES', '5')}) reached. Skipping new signal for {signals[0]['asset']}")
                time.sleep(30)
                continue

            signal = signals[0]
            # Get confidence level
            confidence_level = get_confidence_level(signal['confidence_percent'])
            
            # Only proceed if confidence is moderate or higher
            if confidence_level == "‚ùå WEAK":
                logger.info(f"Skipping weak signal for {signal['asset']}")
                time.sleep(30)
                continue
            
            # --- MICRO BACKTEST CHECK ---
            bt = signal.get('backtest', {})
            
            # Must have positive expectancy and enough trades
            if bt.get("trades", 0) < 15 or bt.get("exp", 0.0) <= MICRO_BACKTEST_MIN_EXPECTANCY:
                logger.info(f"‚ùå {signal['asset']} micro-backtest failed: {bt}")
                # Message to indicate why th ebacktest failed
                message = f"‚ùå {signal['asset']} micro-backtest failed:\n"
                message += f"- Trades: {bt.get('trades', 0)} (min 15)\n"
                message += f"- Expectancy: {bt.get('exp', 0.0):.4f} (min {MICRO_BACKTEST_MIN_EXPECTANCY})\n"
                send_bot_message(int(os.getenv("TELEGRAM_CHAT_ID")), message)
                time.sleep(30)
                continue
            
            logger.info(f"‚úÖ {signal['asset']} micro-backtest passed: {bt}")
            
           
            # --- LIQUIDITY PERSISTENCE CHECK ---
            cex_check = lpm.validate_cex_consensus_for_dex_asset(signal['asset'])
            if cex_check["consensus"] == "NO_CEX_PAIR":
                logger.info(f"üõë {signal['asset']} CEX consensus check failed: {cex_check['reason']}")
                send_bot_message(int(os.getenv("TELEGRAM_CHAT_ID")), f"üõë {signal['asset']} CEX consensus check failed: {cex_check['reason']}")
                time.sleep(30)
                continue
            elif cex_check["consensus"] == "LOW":
                logger.info(f"‚ùå Skipping {signal['asset']}: LOW CEX consensus ({cex_check['reason']})")
                send_bot_message(int(os.getenv("TELEGRAM_CHAT_ID")), f"‚ùå Skipping {signal['asset']}: LOW CEX consensus ({cex_check['reason']})")
                time.sleep(30)
                continue
            else:
                logger.info(f"‚úÖ {signal['asset']} passed CEX consensus: {cex_check['reason']}")
            
            # Analyze with LLM
            logger.info(f"Analyzing signal for {signal['asset']} with LLM...")
            llm_result = analyze_with_llm(signal)
            print(llm_result["approved"])
            if not bool(llm_result["approved"]):
                logger.info(f"LLM rejected signal for {signal['asset']}: {llm_result['analysis'][:200]}...")
                message = f"LLM rejected signal for {signal['asset']}:\n{llm_result['analysis']}"
                send_bot_message(int(os.getenv("TELEGRAM_CHAT_ID")), message)
                time.sleep(30)
                continue
            
            # Parse the JSON from LLM analysis
            try:
                # Extract JSON from code blocks if present
                analysis = llm_result["analysis"]
                if '```json' in analysis:
                    json_start = analysis.find('```json') + 7
                    json_end = analysis.find('```', json_start)
                    if json_end == -1:
                        json_end = len(analysis)
                    json_str = analysis[json_start:json_end].strip()
                else:
                    json_str = analysis.strip()
                
                parsed_signal = json.loads(json_str)
                
                # Ensure required fields are present
                required_fields = ['symbol', 'side', 'entry', 'stop_loss', 'take_profit', 'confidence']
                if not all(field in parsed_signal for field in required_fields):
                    raise ValueError("Missing required fields")
                    
            except (json.JSONDecodeError, KeyError, ValueError) as e:
                logger.warning(f"Failed to parse LLM JSON response for {signal['asset']}: {e}")
                time.sleep(30)
                continue
            
            
            # Execute position using your existing executor
            execution_result = place_futures_order(parsed_signal)
            
            logger.info(f"Execution result for {signal['asset']}: {execution_result}")

        # Sleep for 30 seconds before next fetch
        time.sleep(30)



if __name__ == "__main__":
    # # Check for tables
    initialize_database_tables()

    # Recover the order state on startup
    recover_order_state_on_startup()
    # Start orphan watcher in a separate thread
    start_orphan_watcher(interval_seconds=20)

    # # Start signal processing
    process_signal()